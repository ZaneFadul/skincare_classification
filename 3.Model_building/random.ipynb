{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "344443636c3027c5042750c9c609acdda283a9c43681b128a8c1053e7ad2aa7d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os \n",
    "\n",
    "os.chdir(\"/Users/susanchen/Documents/GitHub/skincare_classification/Plots\")\n",
    "from collections import Counter "
   ]
  },
  {
   "source": [
    "## Building Multi-label Classifiers \n",
    "### Base Classifiers to test: Binary Relevance (no correlation), Binary Relevance (with correlation), Multi-label Random Forest, Multi-label Decision Tree, Classifier Chain, Binary Releveance with Stacking Aggregation, Cross-Coupling Aggregation (to combat class-imbalances)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/susanchen/Documents/GitHub/skincare_classification/Data_sets/Master_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        Product               Brand  \\\n",
       "0                     #InstantDetox Facial Mask            Biobelle   \n",
       "1                           #Peachy Facial Mask            Biobelle   \n",
       "2                       #Rise&Shine Facial Mask            Biobelle   \n",
       "3                       #RoséAllDay Facial Mask            Biobelle   \n",
       "4                       #VitaminSea Facial Mask            Biobelle   \n",
       "...                                         ...                 ...   \n",
       "2019  Youthful Vitamin C Fresh Radiance Essence                 No7   \n",
       "2020                      Yuza Sorbet Day Cream            Erborian   \n",
       "2021                Yuza Sorbet Night Treatment            Erborian   \n",
       "2022               Yuzu Overnight Moisture Mask  Earth Therapeutics   \n",
       "2023                             pHenomenal Gel                Pixi   \n",
       "\n",
       "                                            Ingredients  Price  \\\n",
       "0     Water, Butylene Glycol, Glycerin, Trehalose, H...   3.99   \n",
       "1     Water, Methylpropanediol, Butylene Glycol, Gly...   3.99   \n",
       "2     Water, Glycerin, Butylene Glycol, Triethylhexa...   3.99   \n",
       "3     Water, Methylpropanediol, Glycerin, Propanedio...   3.99   \n",
       "4     Water, Butylene Glycol, Glycerin, Hydroxyaceto...   3.99   \n",
       "...                                                 ...    ...   \n",
       "2019  Aqua (Water), Butylene Glycol, Glycerin, Gluco...  24.99   \n",
       "2020  Aqua/Water, Cyclomethicone, Glycerin, Nylon-12...  48.00   \n",
       "2021  Aqua/Water, Cyclomethicone, Glycerin, Cetearyl...  55.00   \n",
       "2022  Water (Aqua), Propanediol, Glycerin, Hydrogena...   7.00   \n",
       "2023  Aqua/Water/Eau, Glycerin, Glycereth-26, Betain...  24.00   \n",
       "\n",
       "                                              Skin_Type active_ingredient  \\\n",
       "0                                              ['Oily']               NaN   \n",
       "1                                               ['Dry']               NaN   \n",
       "2                                       ['Combination']               NaN   \n",
       "3                                       ['Combination']               NaN   \n",
       "4                                               ['Dry']               NaN   \n",
       "...                                                 ...               ...   \n",
       "2019           ['Combination', 'Dry', 'Normal', 'Oily']               NaN   \n",
       "2020  ['Combination', 'Dry', 'Normal', 'Oily', 'Sens...               NaN   \n",
       "2021  ['Combination', 'Dry', 'Normal', 'Oily', 'Sens...               NaN   \n",
       "2022      ['Combination', 'Dry', 'Normal', 'Sensitive']               NaN   \n",
       "2023                                           ['Oily']               NaN   \n",
       "\n",
       "                                    inactive_ingredient  \\\n",
       "0     Water, Butylene Glycol, Glycerin, Trehalose, H...   \n",
       "1     Water, Methylpropanediol, Butylene Glycol, Gly...   \n",
       "2     Water, Glycerin, Butylene Glycol, Triethylhexa...   \n",
       "3     Water, Methylpropanediol, Glycerin, Propanedio...   \n",
       "4     Water, Butylene Glycol, Glycerin, Hydroxyaceto...   \n",
       "...                                                 ...   \n",
       "2019  Aqua (Water), Butylene Glycol, Glycerin, Gluco...   \n",
       "2020  Aqua/Water, Cyclomethicone, Glycerin, Nylon-12...   \n",
       "2021  Aqua/Water, Cyclomethicone, Glycerin, Cetearyl...   \n",
       "2022  Water (Aqua), Propanediol, Glycerin, Hydrogena...   \n",
       "2023  Aqua/Water/Eau, Glycerin, Glycereth-26, Betain...   \n",
       "\n",
       "     active_ingredient_list  \\\n",
       "0                        []   \n",
       "1                        []   \n",
       "2                        []   \n",
       "3                        []   \n",
       "4                        []   \n",
       "...                     ...   \n",
       "2019                     []   \n",
       "2020                     []   \n",
       "2021                     []   \n",
       "2022                     []   \n",
       "2023                     []   \n",
       "\n",
       "                               inactive_ingredient_list  Is_alphabetical  ...  \\\n",
       "0     ['Water', 'Butylene Glycol', 'Glycerin', 'Treh...            False  ...   \n",
       "1     ['Water', 'Methylpropanediol', 'Butylene Glyco...            False  ...   \n",
       "2     ['Water', 'Glycerin', 'Butylene Glycol', 'Trie...            False  ...   \n",
       "3     ['Water', 'Methylpropanediol', 'Glycerin', 'Pr...            False  ...   \n",
       "4     ['Water', 'Butylene Glycol', 'Glycerin', 'Hydr...            False  ...   \n",
       "...                                                 ...              ...  ...   \n",
       "2019  ['Aqua (Water)', 'Butylene Glycol', 'Glycerin'...            False  ...   \n",
       "2020  ['Aqua/Water', 'Cyclomethicone', 'Glycerin', '...            False  ...   \n",
       "2021  ['Aqua/Water', 'Cyclomethicone', 'Glycerin', '...            False  ...   \n",
       "2022  ['Water (Aqua)', 'Propanediol', 'Glycerin', 'H...            False  ...   \n",
       "2023  ['Aqua/Water/Eau', 'Glycerin', 'Glycereth-26',...            False  ...   \n",
       "\n",
       "     num_of_Emollients num_of_Hydration  num_of_Skin-Restoring  \\\n",
       "0                    0                0                      0   \n",
       "1                    0                0                      0   \n",
       "2                    0                0                      0   \n",
       "3                    0                0                      0   \n",
       "4                    0                0                      0   \n",
       "...                ...              ...                    ...   \n",
       "2019                 0                0                      0   \n",
       "2020                 1                0                      0   \n",
       "2021                 2                0                      0   \n",
       "2022                 2                0                      0   \n",
       "2023                 0                0                      0   \n",
       "\n",
       "      num_of_Plant Extracts  num_of_Preservatives  num_of_Skin-Softening  \\\n",
       "0                         0                     0                      0   \n",
       "1                         0                     0                      0   \n",
       "2                         0                     0                      0   \n",
       "3                         1                     2                      0   \n",
       "4                         1                     0                      0   \n",
       "...                     ...                   ...                    ...   \n",
       "2019                      2                     3                      0   \n",
       "2020                      1                     1                      0   \n",
       "2021                      0                     0                      0   \n",
       "2022                      1                     0                      0   \n",
       "2023                      0                     0                      0   \n",
       "\n",
       "      num_of_Sensitizing  num_of_Skin-Replenishing  \\\n",
       "0                      0                         1   \n",
       "1                      0                         1   \n",
       "2                      0                         1   \n",
       "3                      0                         2   \n",
       "4                      0                         1   \n",
       "...                  ...                       ...   \n",
       "2019                   0                         2   \n",
       "2020                   0                         1   \n",
       "2021                   0                         1   \n",
       "2022                   0                         1   \n",
       "2023                   0                         1   \n",
       "\n",
       "                                                  top_3  \\\n",
       "0          ['Butylene Glycol', 'Glycerin', 'Trehalose']   \n",
       "1     ['Methylpropanediol', 'Butylene Glycol', 'Glyc...   \n",
       "2     ['Glycerin', 'Butylene Glycol', 'Triethylhexan...   \n",
       "3      ['Methylpropanediol', 'Glycerin', 'Propanediol']   \n",
       "4     ['Butylene Glycol', 'Glycerin', 'Hydroxyacetop...   \n",
       "...                                                 ...   \n",
       "2019  ['Butylene Glycol', 'Glycerin', 'Gluconolactone']   \n",
       "2020         ['Cyclomethicone', 'Glycerin', 'Nylon-12']   \n",
       "2021  ['Cyclomethicone', 'Glycerin', 'Cetearyl Alcoh...   \n",
       "2022  ['Propanediol', 'Glycerin', 'Hydrogenated Poly...   \n",
       "2023            ['Glycerin', 'Glycereth-26', 'Betaine']   \n",
       "\n",
       "                                     top3_category_list  \n",
       "0     ['Texture Enhancer', 'Skin-Replenishing, Skin-...  \n",
       "1     [None, 'Texture Enhancer', 'Skin-Replenishing,...  \n",
       "2     ['Skin-Replenishing, Skin-Restoring', 'Texture...  \n",
       "3     [None, 'Skin-Replenishing, Skin-Restoring', None]  \n",
       "4     ['Texture Enhancer', 'Skin-Replenishing, Skin-...  \n",
       "...                                                 ...  \n",
       "2019  ['Texture Enhancer', 'Skin-Replenishing, Skin-...  \n",
       "2020  ['Emollients', 'Skin-Replenishing, Skin-Restor...  \n",
       "2021  ['Emollients', 'Skin-Replenishing, Skin-Restor...  \n",
       "2022  [None, 'Skin-Replenishing, Skin-Restoring', None]  \n",
       "2023  ['Skin-Replenishing, Skin-Restoring', None, None]  \n",
       "\n",
       "[2018 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Brand</th>\n      <th>Ingredients</th>\n      <th>Price</th>\n      <th>Skin_Type</th>\n      <th>active_ingredient</th>\n      <th>inactive_ingredient</th>\n      <th>active_ingredient_list</th>\n      <th>inactive_ingredient_list</th>\n      <th>Is_alphabetical</th>\n      <th>...</th>\n      <th>num_of_Emollients</th>\n      <th>num_of_Hydration</th>\n      <th>num_of_Skin-Restoring</th>\n      <th>num_of_Plant Extracts</th>\n      <th>num_of_Preservatives</th>\n      <th>num_of_Skin-Softening</th>\n      <th>num_of_Sensitizing</th>\n      <th>num_of_Skin-Replenishing</th>\n      <th>top_3</th>\n      <th>top3_category_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#InstantDetox Facial Mask</td>\n      <td>Biobelle</td>\n      <td>Water, Butylene Glycol, Glycerin, Trehalose, H...</td>\n      <td>3.99</td>\n      <td>['Oily']</td>\n      <td>NaN</td>\n      <td>Water, Butylene Glycol, Glycerin, Trehalose, H...</td>\n      <td>[]</td>\n      <td>['Water', 'Butylene Glycol', 'Glycerin', 'Treh...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Butylene Glycol', 'Glycerin', 'Trehalose']</td>\n      <td>['Texture Enhancer', 'Skin-Replenishing, Skin-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#Peachy Facial Mask</td>\n      <td>Biobelle</td>\n      <td>Water, Methylpropanediol, Butylene Glycol, Gly...</td>\n      <td>3.99</td>\n      <td>['Dry']</td>\n      <td>NaN</td>\n      <td>Water, Methylpropanediol, Butylene Glycol, Gly...</td>\n      <td>[]</td>\n      <td>['Water', 'Methylpropanediol', 'Butylene Glyco...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Methylpropanediol', 'Butylene Glycol', 'Glyc...</td>\n      <td>[None, 'Texture Enhancer', 'Skin-Replenishing,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#Rise&amp;Shine Facial Mask</td>\n      <td>Biobelle</td>\n      <td>Water, Glycerin, Butylene Glycol, Triethylhexa...</td>\n      <td>3.99</td>\n      <td>['Combination']</td>\n      <td>NaN</td>\n      <td>Water, Glycerin, Butylene Glycol, Triethylhexa...</td>\n      <td>[]</td>\n      <td>['Water', 'Glycerin', 'Butylene Glycol', 'Trie...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Glycerin', 'Butylene Glycol', 'Triethylhexan...</td>\n      <td>['Skin-Replenishing, Skin-Restoring', 'Texture...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#RoséAllDay Facial Mask</td>\n      <td>Biobelle</td>\n      <td>Water, Methylpropanediol, Glycerin, Propanedio...</td>\n      <td>3.99</td>\n      <td>['Combination']</td>\n      <td>NaN</td>\n      <td>Water, Methylpropanediol, Glycerin, Propanedio...</td>\n      <td>[]</td>\n      <td>['Water', 'Methylpropanediol', 'Glycerin', 'Pr...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>['Methylpropanediol', 'Glycerin', 'Propanediol']</td>\n      <td>[None, 'Skin-Replenishing, Skin-Restoring', None]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#VitaminSea Facial Mask</td>\n      <td>Biobelle</td>\n      <td>Water, Butylene Glycol, Glycerin, Hydroxyaceto...</td>\n      <td>3.99</td>\n      <td>['Dry']</td>\n      <td>NaN</td>\n      <td>Water, Butylene Glycol, Glycerin, Hydroxyaceto...</td>\n      <td>[]</td>\n      <td>['Water', 'Butylene Glycol', 'Glycerin', 'Hydr...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Butylene Glycol', 'Glycerin', 'Hydroxyacetop...</td>\n      <td>['Texture Enhancer', 'Skin-Replenishing, Skin-...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>Youthful Vitamin C Fresh Radiance Essence</td>\n      <td>No7</td>\n      <td>Aqua (Water), Butylene Glycol, Glycerin, Gluco...</td>\n      <td>24.99</td>\n      <td>['Combination', 'Dry', 'Normal', 'Oily']</td>\n      <td>NaN</td>\n      <td>Aqua (Water), Butylene Glycol, Glycerin, Gluco...</td>\n      <td>[]</td>\n      <td>['Aqua (Water)', 'Butylene Glycol', 'Glycerin'...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>['Butylene Glycol', 'Glycerin', 'Gluconolactone']</td>\n      <td>['Texture Enhancer', 'Skin-Replenishing, Skin-...</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>Yuza Sorbet Day Cream</td>\n      <td>Erborian</td>\n      <td>Aqua/Water, Cyclomethicone, Glycerin, Nylon-12...</td>\n      <td>48.00</td>\n      <td>['Combination', 'Dry', 'Normal', 'Oily', 'Sens...</td>\n      <td>NaN</td>\n      <td>Aqua/Water, Cyclomethicone, Glycerin, Nylon-12...</td>\n      <td>[]</td>\n      <td>['Aqua/Water', 'Cyclomethicone', 'Glycerin', '...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Cyclomethicone', 'Glycerin', 'Nylon-12']</td>\n      <td>['Emollients', 'Skin-Replenishing, Skin-Restor...</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>Yuza Sorbet Night Treatment</td>\n      <td>Erborian</td>\n      <td>Aqua/Water, Cyclomethicone, Glycerin, Cetearyl...</td>\n      <td>55.00</td>\n      <td>['Combination', 'Dry', 'Normal', 'Oily', 'Sens...</td>\n      <td>NaN</td>\n      <td>Aqua/Water, Cyclomethicone, Glycerin, Cetearyl...</td>\n      <td>[]</td>\n      <td>['Aqua/Water', 'Cyclomethicone', 'Glycerin', '...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Cyclomethicone', 'Glycerin', 'Cetearyl Alcoh...</td>\n      <td>['Emollients', 'Skin-Replenishing, Skin-Restor...</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>Yuzu Overnight Moisture Mask</td>\n      <td>Earth Therapeutics</td>\n      <td>Water (Aqua), Propanediol, Glycerin, Hydrogena...</td>\n      <td>7.00</td>\n      <td>['Combination', 'Dry', 'Normal', 'Sensitive']</td>\n      <td>NaN</td>\n      <td>Water (Aqua), Propanediol, Glycerin, Hydrogena...</td>\n      <td>[]</td>\n      <td>['Water (Aqua)', 'Propanediol', 'Glycerin', 'H...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Propanediol', 'Glycerin', 'Hydrogenated Poly...</td>\n      <td>[None, 'Skin-Replenishing, Skin-Restoring', None]</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>pHenomenal Gel</td>\n      <td>Pixi</td>\n      <td>Aqua/Water/Eau, Glycerin, Glycereth-26, Betain...</td>\n      <td>24.00</td>\n      <td>['Oily']</td>\n      <td>NaN</td>\n      <td>Aqua/Water/Eau, Glycerin, Glycereth-26, Betain...</td>\n      <td>[]</td>\n      <td>['Aqua/Water/Eau', 'Glycerin', 'Glycereth-26',...</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['Glycerin', 'Glycereth-26', 'Betaine']</td>\n      <td>['Skin-Replenishing, Skin-Restoring', None, None]</td>\n    </tr>\n  </tbody>\n</table>\n<p>2018 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 243
    }
   ],
   "source": [
    "smallClasses = [\"['Oily', 'Sensitive']\", \"['Dry', 'Normal', 'Oily']\", \"['Dry', 'Oily']\", \"['Dry', 'Normal', 'Oily', 'Sensitive']\"]\n",
    "data = data[data.Skin_Type != \"['Oily', 'Sensitive']\"]\n",
    "data = data[data.Skin_Type != \"['Dry', 'Normal', 'Oily']\"]\n",
    "data = data[data.Skin_Type != \"['Dry', 'Oily']\"]\n",
    "data = data[data.Skin_Type != \"['Dry', 'Normal', 'Oily', 'Sensitive']\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({\"['Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\": 918, \"['Combination', 'Dry', 'Normal', 'Oily']\": 239, \"['Combination', 'Normal', 'Oily']\": 113, \"['Combination', 'Oily']\": 91, \"['Normal']\": 83, \"['Dry', 'Normal']\": 72, \"['Dry']\": 71, \"['Combination', 'Dry', 'Normal']\": 66, \"['Combination', 'Dry', 'Normal', 'Sensitive']\": 61, \"['Combination']\": 59, \"['Sensitive']\": 46, \"['Combination', 'Normal']\": 30, \"['Dry', 'Normal', 'Sensitive']\": 29, \"['Oily']\": 26, \"['Combination', 'Normal', 'Oily', 'Sensitive']\": 23, \"['Combination', 'Dry']\": 22, \"['Dry', 'Sensitive']\": 19, \"['Combination', 'Dry', 'Oily']\": 13, \"['Normal', 'Sensitive']\": 12, \"['Combination', 'Oily', 'Sensitive']\": 10, \"['Combination', 'Dry', 'Sensitive']\": 7, \"['Combination', 'Dry', 'Oily', 'Sensitive']\": 4, \"['Normal', 'Oily']\": 4})\n"
     ]
    }
   ],
   "source": [
    "X = data[data.columns[12:23]].values\n",
    "y = data.Skin_Type\n",
    "#['Oily', 'Sensitive']\": 2, \"['Dry', 'Normal', 'Oily']\": 2, \"['Dry', 'Oily']\": 1, \"['Dry', 'Normal', 'Oily', 'Sensitive']\": 1\n",
    "#summarize class distribution \n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({\"['Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\": 918, \"['Oily']\": 300, \"['Dry']\": 300, \"['Combination']\": 300, \"['Dry', 'Sensitive']\": 300, \"['Sensitive']\": 300, \"['Combination', 'Oily']\": 300, \"['Combination', 'Normal', 'Oily']\": 300, \"['Dry', 'Normal']\": 300, \"['Combination', 'Dry']\": 300, \"['Combination', 'Dry', 'Oily']\": 300, \"['Normal']\": 300, \"['Combination', 'Dry', 'Normal', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Normal']\": 300, \"['Dry', 'Normal', 'Sensitive']\": 300, \"['Combination', 'Normal']\": 300, \"['Combination', 'Normal', 'Oily', 'Sensitive']\": 300, \"['Normal', 'Sensitive']\": 300, \"['Normal', 'Oily']\": 300, \"['Combination', 'Dry', 'Normal', 'Oily']\": 239})\nCounter({\"['Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\": 800, \"['Combination', 'Dry', 'Normal', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Normal']\": 300, \"['Combination', 'Dry', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Oily']\": 300, \"['Combination', 'Dry', 'Sensitive']\": 300, \"['Combination', 'Dry']\": 300, \"['Combination', 'Normal', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Normal', 'Oily']\": 300, \"['Combination', 'Normal']\": 300, \"['Combination', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Oily']\": 300, \"['Combination']\": 300, \"['Dry', 'Normal', 'Sensitive']\": 300, \"['Dry', 'Normal']\": 300, \"['Dry', 'Sensitive']\": 300, \"['Dry']\": 300, \"['Normal', 'Oily']\": 300, \"['Normal', 'Sensitive']\": 300, \"['Normal']\": 300, \"['Oily']\": 300, \"['Sensitive']\": 300, \"['Combination', 'Dry', 'Normal', 'Oily']\": 239})\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "# transform the dataset\n",
    "oversample = SMOTE(k_neighbors=3, sampling_strategy = {\"['Combination', 'Normal', 'Oily']\": 300, \"['Combination', 'Oily']\": 300, \"['Normal']\": 300, \"['Dry', 'Normal']\": 300, \"['Dry']\": 300, \"['Combination', 'Dry', 'Normal']\": 300, \"['Combination', 'Dry', 'Normal', 'Sensitive']\": 300, \"['Combination']\": 300, \"['Sensitive']\": 300, \"['Combination', 'Normal']\": 300, \"['Dry', 'Normal', 'Sensitive']\": 300, \"['Oily']\": 300, \"['Combination', 'Normal', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Dry']\": 300, \"['Dry', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Oily']\": 300, \"['Normal', 'Sensitive']\": 300, \"['Combination', 'Oily', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Sensitive']\": 300, \"['Combination', 'Dry', 'Oily', 'Sensitive']\": 300, \"['Normal', 'Oily']\": 300})\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy= {\"['Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\": 800})\n",
    "X, y = under.fit_resample(X, y)\n",
    "\n",
    "\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({\"['Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\": 918, \"['Combination', 'Dry', 'Normal', 'Oily']\": 239, \"['Combination', 'Normal', 'Oily']\": 113, \"['Combination', 'Oily']\": 91, \"['Normal']\": 83, \"['Dry', 'Normal']\": 72, \"['Dry']\": 71, \"['Combination', 'Dry', 'Normal']\": 66, \"['Combination', 'Dry', 'Normal', 'Sensitive']\": 61, \"['Combination']\": 59, \"['Sensitive']\": 46, \"['Combination', 'Normal']\": 30, \"['Dry', 'Normal', 'Sensitive']\": 29, \"['Oily']\": 26, \"['Combination', 'Normal', 'Oily', 'Sensitive']\": 23, \"['Combination', 'Dry']\": 22, \"['Dry', 'Sensitive']\": 19, \"['Combination', 'Dry', 'Oily']\": 13, \"['Normal', 'Sensitive']\": 12, \"['Combination', 'Oily', 'Sensitive']\": 10, \"['Combination', 'Dry', 'Sensitive']\": 7, \"['Combination', 'Dry', 'Oily', 'Sensitive']\": 4, \"['Normal', 'Oily']\": 4, \"['Oily', 'Sensitive']\": 2, \"['Dry', 'Normal', 'Oily']\": 2, \"['Dry', 'Oily']\": 1, \"['Dry', 'Normal', 'Oily', 'Sensitive']\": 1})\n"
     ]
    }
   ],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=\"minority\", random_state=6354)\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_under, y_under = oversample.fit_resample(X_over, y_over)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset sizes:\n\tTrain (4403, 11)\n\tVal (1468, 11)\n\tTest (1468, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#X = data[data.columns[12:23]].values \n",
    "#y = data.Skin_Type\n",
    "y = MultiLabelBinarizer().fit_transform(y)\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, shuffle=True, test_size=0.4, random_state = 1024)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, shuffle=False, test_size=0.5, random_state = 1024)\n",
    "\n",
    "print(\"Dataset sizes:\\n\\tTrain %s\\n\\tVal %s\\n\\tTest %s\" % (X_train.shape, X_val.shape,X_test.shape))"
   ]
  },
  {
   "source": [
    "## Binary Relevance with Gaussian "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.03337874659400545\nF1 Score =  0.6904925652761639\nHamming Loss 0.3865359554555147\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.metrics as metrics\n",
    "# initialize binary relevance multi-label classifier with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = GaussianNB(),\n",
    "    require_dense=[True, True] \n",
    ")\n",
    "# train\n",
    "classifier.fit(X_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(X_val)\n",
    "# accuracy\n",
    "print(\"Accuracy = \", metrics.accuracy_score(y_val,predictions))\n",
    "print(\"F1 Score = \", metrics.f1_score(y_val,predictions, average = \"micro\"))\n",
    "print(\"Hamming Loss\", metrics.hamming_loss(y_val, predictions))\n"
   ]
  },
  {
   "source": [
    "## One vs the Rest Classifier "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.03201634877384196\nF1 Score =  0.8391369447537144\nHamming Loss 0.24819334202108756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "classif = OneVsRestClassifier(SVC(kernel='linear', random_state= 1024))\n",
    "classif.fit(X_train, y_train)\n",
    "val_pred = classif.predict(X_val)\n",
    "# accuracy\n",
    "print(\"Accuracy = \", metrics.accuracy_score(y_val, val_pred))\n",
    "print(\"F1 Score = \", metrics.f1_score(y_val,val_pred, average = \"micro\"))\n",
    "print(\"Hamming Loss\", metrics.hamming_loss(y_val, val_pred))"
   ]
  },
  {
   "source": [
    "## Decision Tree Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_depth': [5, 6, 7, 8, 9],\n",
       "                         'max_leaf_nodes': [5, 10, 15]},\n",
       "             scoring='accuracy')"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "dt =  DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'max_depth' :[5,6,7,8,9],\n",
    "    'max_leaf_nodes': [5,10,15]\n",
    "}\n",
    "gs = GridSearchCV(dt, param_grid, scoring = \"accuracy\", cv= 5)\n",
    "\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8974625623960066\n"
     ]
    }
   ],
   "source": [
    "tree = dt.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(metrics.f1_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "source": [
    "  ## Binary Relevance with Random Forest Classifier "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training time taken:  28.0 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time \n",
    "start=time.time()\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print('training time taken: ',round(time.time()-start,0),'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binary Relevance accuracy: 0.046\nBinary Relevance F1-score: 0.692\nBinary Relevance Hamming Loss: 0.379\n"
     ]
    }
   ],
   "source": [
    "y_hat=classifier.predict(X_test)\n",
    "a=metrics.accuracy_score(y_test, y_hat)\n",
    "br_f1=metrics.f1_score(y_test, y_hat, average='micro')\n",
    "br_hamm=metrics.hamming_loss(y_test,y_hat)\n",
    "\n",
    "print('Binary Relevance accuracy:',round(a,3))\n",
    "print('Binary Relevance F1-score:',round(br_f1,3))\n",
    "print('Binary Relevance Hamming Loss:',round(br_hamm,3))"
   ]
  },
  {
   "source": [
    "## Label Powerset with Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training time taken:  2.0 seconds\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "classifier = LabelPowerset(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "start=time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "print('training time taken: ',round(time.time()-start,0),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binary Relevance accuracy: 0.04\nLabel Powerset F1-score: 0.69\nLabel Powerset Hamming Loss: 0.383\n"
     ]
    }
   ],
   "source": [
    "y_hat=classifier.predict(X_test)\n",
    "lp_a=metrics.accuracy_score(y_test, y_hat)\n",
    "lp_f1=metrics.f1_score(y_test, y_hat, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(y_test,y_hat)\n",
    "print('Binary Relevance accuracy:',round(lp_a,3))\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))"
   ]
  },
  {
   "source": [
    "## Multi-label K Nearest Neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training time taken:  132.0 seconds\nbest parameters : {'k': 2, 's': 0.5} best score:  0.9003037410622218\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "import time\n",
    "\n",
    "parameters = {'k': range(1,3), \n",
    "              's': [0.5, 0.7, 1.0]}\n",
    "\n",
    "score = 'f1_micro'\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "classifier = GridSearchCV(MLkNN(), parameters, scoring=score)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print('training time taken: ',round(time.time()-start,0),'seconds')\n",
    "print('best parameters :', classifier.best_params_, 'best score: ',\n",
    "      classifier.best_score_)"
   ]
  },
  {
   "source": [
    "## "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Binary Relevance K-Nearest Neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training time taken:  3.0 seconds\nbest parameters : {'k': 3} best score:  0.8856294650874726\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import BRkNNaClassifier\n",
    "\n",
    "parameters = {'k': range(3,5)}\n",
    "score = 'f1_samples'\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "classifier = GridSearchCV(BRkNNaClassifier(), parameters, scoring=score)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print('training time taken: ',round(time.time()-start,0),'seconds')\n",
    "print('best parameters :', classifier.best_params_,\n",
    "      'best score: ',classifier.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5674386920980926\n"
     ]
    }
   ],
   "source": [
    "BrKnn = BRkNNaClassifier(k= 3)\n",
    "BrKnn.fit(X_train, y_train)\n",
    "pred = BrKnn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio= 0.1\n",
    "\n",
    "def mLIFT(train_data,train_target,test_data,test_target,ratio):\n",
    "    num_train, dim= train_data.shape\n",
    "    num_test, num_class= test_target.shape\n",
    "    \n",
    "    P_Centers= []\n",
    "    N_Centers= []\n",
    "    \n",
    "    ##### KMeans, and save the centers\n",
    "    for i in range(num_class):\n",
    "        print (\"Performing clustering:%d/%d\" %(i+ 1, num_class))\n",
    "    \n",
    "        p_data= train_data[train_target[:,i]== 1]\n",
    "        n_data= train_data[train_target[:,i]== -1]\n",
    "    \n",
    "        k1= int(min( math.ceil(p_data.shape[0]* ratio), math.ceil(n_data.shape[0]* ratio) ))\n",
    "        #print(\"k1= k2= %d\" %k1)\n",
    "        k2= k1;\n",
    "    \n",
    "        if(k1== 0):\n",
    "            POS_C=[]\n",
    "            zero_kmeans= KMeans(n_clusters= min(50, num_train)).fit(train_data)\n",
    "            NEG_C= zero_kmeans.cluster_centers_\n",
    "        else:\n",
    "            # Positive\n",
    "            if(p_data.shape[0]== 1):\n",
    "                POS_C= p_data\n",
    "            else:\n",
    "                p_kmeans= KMeans(n_clusters= k1).fit(p_data)\n",
    "                POS_C= p_kmeans.cluster_centers_\n",
    "            # Negative\n",
    "            if(n_data.shape[0]== 1):\n",
    "                NEG_C= n_data\n",
    "            else:\n",
    "                n_kmeans= KMeans(n_clusters= k2).fit(n_data)\n",
    "                NEG_C= n_kmeans.cluster_centers_\n",
    "                \n",
    "        # Save the cluster centers\n",
    "        P_Centers.append(POS_C)\n",
    "        N_Centers.append(NEG_C)\n",
    "            \n",
    "    #print(\"The size of P_Canters is %d\\n\" %len(P_Centers))\n",
    "    \n",
    "    ##### Do the map and save the models\n",
    "    Models= []\n",
    "    for i in range(num_class):\n",
    "        print (\"Building classifiers: :%d/%d\" %(i+ 1, num_class))\n",
    "        centers= np.vstack((P_Centers[i], N_Centers[i]))\n",
    "        num_center= centers.shape[0]\n",
    "        # print(num_center)\n",
    "        data= []\n",
    "    \n",
    "        if(num_center>= 5000):\n",
    "            print(\"Too many cluster center!\")\n",
    "            break\n",
    "        else:\n",
    "            blocksize= 5000- num_center\n",
    "            num_block= int(math.ceil(num_train/ blocksize))\n",
    "            # print(num_block)\n",
    "            \n",
    "            mFirst= True\n",
    "            for j in range(num_block- 1):\n",
    "                print(j)\n",
    "                low= j* blocksize\n",
    "                high= (j+ 1)* blocksize\n",
    "                # Calculate the distance\n",
    "                for k in range(num_center):\n",
    "                    diff= train_data[low:high, :]- centers[k]\n",
    "                    Eu_diff= np.linalg.norm(diff, axis=1)\n",
    "                    if(mFirst== True):\n",
    "                        mFirst= False\n",
    "                        data_temp= Eu_diff\n",
    "                    else:\n",
    "                        data_temp= np.vstack((data_temp, Eu_diff))\n",
    "                    \n",
    "            \n",
    "            low= (num_block- 1)* blocksize\n",
    "            high= num_train\n",
    "            \n",
    "            # Calculate the distance\n",
    "            for j in range(num_center):\n",
    "                diff= train_data[low:high,:]- centers[j]\n",
    "                Eu_diff= np.linalg.norm(diff, axis=1)\n",
    "                if(mFirst== True):\n",
    "                    mFirst= False\n",
    "                    data_temp= Eu_diff\n",
    "                else:\n",
    "                    data_temp= np.vstack((data_temp, Eu_diff))\n",
    "            \n",
    "            data= data_temp.T\n",
    "        \n",
    "        training_instance_matrix= data\n",
    "        training_label_vector= train_target[:,i]\n",
    "    \n",
    "        model_this= SVC(C= 10, probability=True).fit(training_instance_matrix, training_label_vector)\n",
    "        #model_this= LogisticRegression(C= 0.03).fit(training_instance_matrix, training_label_vector)\n",
    "        #model_this= DecisionTreeClassifier().fit(training_instance_matrix, training_label_vector)\n",
    "        #model_this = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         #algorithm=\"SAMME\",\n",
    "                         #n_estimators=50, learning_rate=0.8).fit(training_instance_matrix, training_label_vector)\n",
    "        Models.append(model_this)\n",
    "    \n",
    "    ##### Predict\n",
    "    for i in range(num_class):\n",
    "        print (\"Predicting: :%d/%d\" %(i+ 1, num_class))\n",
    "        centers= np.vstack((P_Centers[i], N_Centers[i]))\n",
    "        num_center= centers.shape[0]\n",
    "        # print(num_center)\n",
    "        data= []\n",
    "    \n",
    "        if(num_center>= 5000):\n",
    "            print(\"Too many cluster center!\")\n",
    "            break\n",
    "        else:\n",
    "            blocksize= 5000- num_center\n",
    "            num_block= int(math.ceil(num_test/ blocksize))\n",
    "            # print(num_block)\n",
    "            \n",
    "            mFirst= True\n",
    "            for j in range(num_block- 1):\n",
    "                print(j)\n",
    "                low= j* blocksize\n",
    "                high= (j+ 1)* blocksize\n",
    "                # Calculate the distance\n",
    "                for k in range(num_center):\n",
    "                    diff= test_data[low:high, :]- centers[k]\n",
    "                    Eu_diff= np.linalg.norm(diff, axis=1)\n",
    "                    if(mFirst== True):\n",
    "                        mFirst= False\n",
    "                        data_temp= Eu_diff\n",
    "                    else:\n",
    "                        data_temp= np.vstack((data_temp, Eu_diff))\n",
    "                    \n",
    "            \n",
    "            low= (num_block- 1)* blocksize\n",
    "            high= num_train\n",
    "            \n",
    "            # Calculate the distance\n",
    "            for j in range(num_center):\n",
    "                diff= test_data[low:high,:]- centers[j]\n",
    "                Eu_diff= np.linalg.norm(diff, axis=1)\n",
    "                if(mFirst== True):\n",
    "                    mFirst= False\n",
    "                    data_temp= Eu_diff\n",
    "                else:\n",
    "                    data_temp= np.vstack((data_temp, Eu_diff))\n",
    "            \n",
    "            data= data_temp.T\n",
    "            # print(data.shape)\n",
    "    \n",
    "        testing_instance_matrix= data;\n",
    "        testing_label_vector= test_target[:, i]\n",
    "    \n",
    "        predicted_label= Models[i].predict(testing_instance_matrix)\n",
    "        \n",
    "        #print (predicted_label)\n",
    "\n",
    "        print(\"The accuracy is: %f\" %accuracy_score(testing_label_vector, predicted_label))\n",
    "        #print(roc_auc_score(testing_label_vector, predicted_label))\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "mLIFT(train_data,train_target,test_data,test_target,ratio);"
   ]
  }
 ]
}